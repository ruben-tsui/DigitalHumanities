{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from urllib import request, parse\n",
    "import re, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_LINKS = set()\n",
    "fon = 'bio300to510AD_Century.txt'\n",
    "fo  = open(fon, \"w\", encoding=\"utf-8\", newline=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'zh.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centuries = [(int(n/100)+1) for n in list(range(300, 511, 100))]\n",
    "centuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh.wikipedia.org/wiki/Category:4世紀出生\n",
      "zh.wikipedia.org/wiki/Category:5世紀出生\n",
      "zh.wikipedia.org/wiki/Category:6世紀出生\n"
     ]
    }
   ],
   "source": [
    "for century in centuries:\n",
    "    url = fr\"{base_url}/wiki/Category:{century}世紀出生\"\n",
    "    print(url)\n",
    "    req  = request.Request('http://'+parse.quote(url), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    try:\n",
    "        page = request.urlopen(req)\n",
    "    except:\n",
    "        continue\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    nodes = soup.find_all('a', {'title':re.compile(r'[一-鿕]+'), 'href':re.compile(r'/wiki/%')}, recursive=True, limit=2000)\n",
    "    for child in nodes:\n",
    "        link = child['href']\n",
    "        if link.startswith('/wiki'):\n",
    "            article = f\"{base_url}{child['href']}\"\n",
    "            article = article.replace(r'/wiki/', r'/zh-tw/')\n",
    "            fo.write(f\"{article}\\n\")\n",
    "            BIO_LINKS.add(article)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fon = 'bio300to510AD_Century_initial_text.txt'\n",
    "fo  = open(fon, \"w\", encoding=\"utf-8\", newline=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "Done! Total time elapsed: [328.2433] sec(s)\n"
     ]
    }
   ],
   "source": [
    "BeginTime = time.time()\n",
    "\n",
    "cnt = 0\n",
    "for link in BIO_LINKS:\n",
    "    cnt += 1\n",
    "    if cnt % 50 == 0:\n",
    "        fo.flush()\n",
    "        print(cnt)\n",
    "    fo.write(link + \"\\n\")\n",
    "    req  = request.Request(f\"http://{link}\", headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    try:\n",
    "        page = request.urlopen(req)\n",
    "    except:\n",
    "        continue\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    headword = ''\n",
    "    h1s = soup.find_all('h1')\n",
    "    if len(h1s) > 0:\n",
    "        headword = h1s[0].text\n",
    "    fo.write(headword + \"\\n\")\n",
    "        \n",
    "    nodes = soup.find_all('div', {'class':'mw-parser-output'})\n",
    "    ## in each \"div\", the first <p></p> seems to contain the information (name, YOB, YOD, courtesy name, place names) \n",
    "    \n",
    "    if len(nodes) > 0:\n",
    "        #print(len(nodes))\n",
    "        div = nodes[0]\n",
    "        basic_info = list(div.find_all('p', recursive=False))\n",
    "        #print(f\"No. of <p> nodes: {len(basic_info)}\")\n",
    "        for paragraph in basic_info:  # <p></p>\n",
    "            children = list(paragraph.children)\n",
    "            if children[0].name == 'a': # if this <p> node contains a URL link, skip\n",
    "                print(f\"Skipping node {str(paragraph)}\")\n",
    "                continue\n",
    "            else:\n",
    "                if isinstance(paragraph, NavigableString):\n",
    "                    fo.write(paragraph.strip() + \"\\n\")\n",
    "                else:\n",
    "                    fo.write(paragraph.text.strip() + \"\\n\")\n",
    "                break # done!\n",
    "        fo.write('-'*20 + \"\\n\")\n",
    "    else:\n",
    "        fo.write(\"No info available for this entry\\n\")\n",
    "    #if cnt > 50:\n",
    "    #    break\n",
    "\n",
    "fo.close()\n",
    "EndTime = time.time()\n",
    "ElapsedTime = EndTime - BeginTime\n",
    "print(\"Done! Total time elapsed: [%.4f] sec(s)\" % ElapsedTime) if True else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
